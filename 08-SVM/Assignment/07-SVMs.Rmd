---
title: "Assignment 7: Support Vector Machines"
author: "Stat 434"
output: rmdformats::readthedown
---

```{r, include = FALSE}
templar::versions()
```

```{r, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval = FALSE)
set.seed(61628)
```


%%%
version: instructions

# Instructions

You will submit an HTML document to Canvas as your final version.

You may work with **one** other person for this assignment.  Make sure both your names are on the HTML document, and you should **both** upload a copy of the assignment to Canvas.

Your document should show your code chunks/cells as well as any output.  Make sure that only relevant output is printed.  Do not, for example, print the entire dataset in your final knitted file.

Your document should also be clearly organized, so that it is easy for a reader to find your answers to each question.

There may be a small penalty for submissions that are difficult to read or navigate.

%%%

Libraries needed for this lab:

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
```




```{r, version = "none", include = FALSE, eval = FALSE}
zoo_orig <- read_csv(here::here("data", "zoo.csv"))

zoo <- class %>%
  select(Class_Number, Class_Type) %>%
  right_join(zoo_orig, by = c(Class_Number = "class_type")) %>%
  select(-Class_Number) %>%
  mutate(
    Class_Type = fct_lump_n(Class_Type, 4)
  ) %>%
  filter(animal_name != "girl")

zoo %>%
  write_csv(here::here("data", "zoo_final.csv"))
  
  
```


# Dataset:  Zoo animals

This week's dataset contains information about animals commonly found at a zoo. The data contains dummy variables for several features that an animal might have, such as:

* Is the animal a predator?
* Does the animal produce milk?
* Is the animal aquatic?

There is also one quantitative variable, `legs` stating how many legs the animal has.



Run the following code to read the data and convert the predictors into scaled matrix form:

```{r}
zoo <- read_csv("https://www.dropbox.com/s/kg89g2y3tp6p9yh/zoo_final.csv?dl=1")
zoo_matrix <- zoo %>%
  select(-Class_Type, -animal_name) %>%
  as.matrix() 
```




# Part One: PCA preprocessing

#### Q1: PCA

Apply a PCA transformation to the matrix version of the data.  Interpret the results - which variables are most important in spreading the observations?  What do each of the first three PCs seem to represent?


```{r, version = "answer_key"}
zoo_pc <- prcomp(zoo_matrix)
zoo_pc$rotation[, 1:3]
```

%%%
version: answer_key

PC1 seems to involve fins versus hair, as well as how many legs the animal has.

PC2 is related to hair, eggs, milk, and size - this makes me think it is finding mammals.

PC3 is about being airborne versus aquatic, and being a predator, so it might separate birds from reptiles.

%%%

#### Q2: Choosing PCs

Look at the percent of variance explained by each PC.  Decide on a number of PCs that you wish to include.



```{r, version = "answer_key"}
cumsum(zoo_pc$sdev^2)/sum(zoo_pc$sdev^2)
```
#### Q3: New dataset

Since PCA is a data processing tool, we can instead apply it as part of a `step_` function in tidymodels.

The `step_pca()` addition to your recipe will automatically include the PCA process in your data pipeline, choosing the number of PCs necessary to reach 80% of the total origial variance.

The `update_role()` addition to your recipe will assign the animal names to be an "id" column, so that the models don't use that variable in the classification process.

Adjust the code below to complete this recipe:

```{r, version = "instructions", eval = FALSE}
zoo_rec <- 
  ### Recipe code here
  update_role(????, new_role = "????") %>%
  step_pca(all_numeric(), threshold = 0.8, 
           options = c(center = TRUE))

```

The `prep()` step is then used to prepare by computing the PCs, and the `bake()` function is then used to make a new dataframe with the chosen PCs as columns.

```{r, version = "instructions", eval = FALSE}
zoo_trained <- zoo_rec %>% prep(zoo)
zoo_pcs <- zoo_trained %>% bake(zoo)
```

```{r, version = "answer_key"}
zoo_rec <- 
  recipe(Class_Type ~ ., data = zoo) %>%
  update_role(animal_name, new_role = "id") %>%
  step_pca(all_numeric(), threshold = 0.8, 
           options = c(center = TRUE))

zoo_trained <- zoo_rec %>% prep(zoo)

zoo_pcs <- zoo_trained %>% bake(zoo)
```

#### Q4: Explore

To verify that the above process worked, plot your observations in the first two PC dimensions, colored by the animal type. 

Then plot your observations in PC2 and PC3, colored by animal type.

Comment on the plots.  Why are certain animal types grouped the way that they are?

```{r, version = "answer_key"}
zoo_pcs %>%
  ggplot(aes(x = PC1, y = PC2, color = Class_Type)) +
  geom_point()
```

## Part Two: SVM

*In this section, if your tuning code will not run, add `error = TRUE` to the chunks that don't work, so that I can see your code.  Then pick an arbitrary (but reasonable) value for `cost` and fit your model using that value to answer the other questions.*

#### Q1: Linear

Create a Support Vector Classifier (aka, an SVM with no kernel) that classifies animal type based on the first three PCs.

Report appropriate metrics of your classifier.

```{r, version = "answer_key"}
svm_1_tune <- svm_poly(cost = tune(), degree = 1) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

zoo_rec <- 
  recipe(Class_Type ~ ., data = zoo) %>%
  update_role(animal_name, new_role = "id") %>%
  step_pca(all_numeric(), num_comp = 3,
           options = c(center = TRUE))

zoo_wflow_1 <- workflow() %>%
  add_recipe(zoo_rec) %>%
  add_model(svm_1_tune)

zoo_cv <- vfold_cv(zoo, v = 3, strata = Class_Type)

zoo_tune_1 <- zoo_wflow_1 %>%
  tune_grid(
    resamples = zoo_cv,
    grid = grid_regular(cost(), levels = 3)
  )

zoo_tune_1 %>% collect_metrics()
zoo_tune_1 %>% show_best(metric = "accuracy")
```

#### Q2: SVM

Repeat Q1, this time for a full Support Vector Machine with a polynomial kernel.

*(You may use the same `cost` you chose in Q1.)*

```{r, version = "answer_key"}
svm_2_tune <- svm_poly(cost = 32, degree = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

zoo_wflow_2 <- workflow() %>%
  add_recipe(zoo_rec) %>%
  add_model(svm_2_tune)

zoo_tune_2 <- zoo_wflow_2 %>%
  tune_grid(
    resamples = zoo_cv,
    grid = grid_regular(degree(), levels = 3)
  )

zoo_tune_2 %>% collect_metrics()
zoo_tune_2 %>% show_best(metric = "accuracy")
```

#### Q3: Interpretation

Explain intuitively why your polynomial SVM had better accuracy than your ordinary linear one.


## Part Three: Prediction

How would your models classify a human being?

Alter the code below to make a dataset with one observation, representing a human.  Use your favorite model from Part Two, Q1 and your favorite model from Part Two, Q2 to predict the animal type of a human.  

Instead of finding only the first predicted type, show the *probabilities* of each category. (Recall: the `type = "prob"` option in the `predict()` function)

*(Hint: the `catsize` variable means "bigger than a cat")*

```{r, version = "instructions", eval = FALSE}
human <- data.frame(
  animal_name = "human",
  hair = 0,
  feathers = 0,
  eggs = 0,
  milk = 0, 
  airborne = 0,
  aquatic = 0,
  predator = 0,
  toothed = 0,
  backbone = 0,
  breathes = 0,
  venomous = 0,
  fins = 0,
  legs = 0,
  tail = 0,
  domestic = 0,
  catsize = 0
)
```

```{r, version = "answer_key"}
human <- data.frame(
  animal_name = "human",
  hair = 1,
  feathers = 0,
  eggs = 0,
  milk = 1, 
  airborne = 0,
  aquatic = 0,
  predator = 1,
  toothed = 1,
  backbone = 1,
  breathes = 1,
  venomous = 0,
  fins = 0,
  legs = 2,
  tail = 0,
  domestic = 1,
  catsize = 1
)

svm_1 <- svm_poly(cost = 32, degree = 1) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

zoo_wflow_1 <- zoo_wflow_1 %>%
  update_model(svm_1)

zoo_wflow_1 %>%
  fit(zoo) %>%
  predict(human, type = "prob")
```

```{r, version = "answer_key"}
svm_2 <- svm_poly(cost = 32, degree = 2) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

zoo_wflow_2 <- zoo_wflow_2 %>%
  update_model(svm_2)

zoo_wflow_2 %>%
  fit(zoo) %>%
  predict(human, type = "prob")
```

## Challenge: Full data

Recall that PCA has two purposes:

1. Reduce the dimensionality for interpretability reasons.

2. Remove "noise" that is in the lower PCs, for better prediction power.

In this lab, we mainly used PCA for Goal #1.  It was easier to visualize our animal types in the first couple PC dimensions.

But did it also help us in Goal #2?  

Try to fit an SVM classifier using the original data, rather than the PCA transformed/reduced version.  Is this better or worse than the model in the lab?

