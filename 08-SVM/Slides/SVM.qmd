---
title: "Classification with Support Vector Machines"
format: 
  revealjs:
    theme: [../../style.scss, dark]
editor: visual
embed-resources: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(tidymodels)
library(kknn)
library(glmnet)
library(discrim)
```

```{r}
#| label: read-clean-data
#| echo: false

# Read data
fed <- read.csv(here::here("06-Dimension-Reduction", 
                           "Slides", 
                           "PCA", 
                           "data", 
                           "federalist.txt")
                ) %>% 
  select(-X) 

# Keep numeric section only
fed_clean <- fed %>% 
  mutate(across(.cols = -Author, .fns = ~ as.numeric(.x)
                )
         )

auths <- fed$Author

# Data from papers with known authors
fed_known <- fed_clean %>% 
  filter(Author != "DIS")

auths_known <- fed_known$Author

# Data from papers with unknown authors
fed_unknown <- fed_clean %>% 
  filter(Author == "DIS")

fed_matrix <- fed_known %>% 
  select(-Author)
```

## Midterm Exam Updates

::: {.incremental}
- Concept exams have been graded and released
- The letter grade associated with your exam is your grade *without* revisions
- You are permitted to submit *one* round of revisions to earn a higher grade
- Revisions are due by Friday, December 15. 
:::

. . .

::: {.callout-tip}
# Take-Home Analysis

The data analysis portion of the exam will be graded no later than this Friday (November 17).
:::

## Midterm Reflections

- Revisions must include *thoughtful* reflection on why your previous answer was incorrect and how your revised solution remedies these issues. 

- Revisions and reflections will be submitted to the `Midterm Part 1 Grade & Revisions` Canvas assignment. 

## Lab Revision Updates

- Revisions on lab assignments will be accepted until Friday, December 15. 

- For the next three weeks, you are permitted to submit *up to one (1)* revision per week.  

- Remember, you are required to earn a "Satisfactory" on *all but 2* assignments to earn a B in the course. 

# Maximal Margin Classifier

## Federalist Papers 

```{r}
#| label: federalist-pc
#| echo: false

pc <- prcomp(fed_matrix, 
             center = TRUE, 
             scale = TRUE)

fed_pca_df <- pc$x %>%
  as.data.frame() %>%
  mutate(
    author = auths_known
  )

fed_pca_jj <- fed_pca_df %>% 
  mutate(author_jj = if_else(author == "JJ", 
                             "John Jay", 
                             "Not John Jay"
                             )
         )

fed_pca_ah <- fed_pca_df %>%
  mutate(author_ah = if_else(author == "AH", 
                             "Hamilton", 
                             "Not Hamilton"
                             ), 
         author_ah = as.factor(author_ah)
         ) 

fed_pca_df_uk <- fed_unknown %>% 
  select(-Author) %>% 
  scale() %>% 
  as_tibble() %>%
  mutate(your = na_if(your, NaN), 
         your = if_else(is.na(your), 
                        0,
                        your)
           ) %>% 
  as.matrix() %*%
  pc$rotation %>%
  as_tibble() %>%
  mutate(
    author = "Unknown"
  )

fed_pca_all  <- bind_rows(fed_pca_df, fed_pca_df_uk)

```

```{r}
#| label: plot-of-pcs-with-known-authors
#| echo: false

fed_pca_jj %>%
  ggplot(mapping = aes(x = PC1, y = PC2, color = author_jj)) +
  geom_point() + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Authors")
```


## A Simple(r) Classification

Suppose we are interested in classifying a new observation as "John Jay" or "Not John Jay".

```{r}
#| label: plot-of-pcs-comparing-jj
#| echo: false

fed_pca_df %>%
  mutate(author_jj = if_else(author == "JJ", 
                             "John Jay", 
                             "Not John Jay"
                             )
         ) %>% 
  ggplot(mapping = aes(x = PC1, y = PC2, color = author_jj)) +
  geom_point() + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Authors")
```

## Choosing a Line

There are many lines we could draw that split the *training* data perfectly between John Jay and not John Jay! 

```{r}
#| label: plot-of-pcs-comparing-lines
#| echo: false

fed_pca_jj %>% 
  ggplot(mapping = aes(x = PC1, y = PC2, color = author_jj)) +
  geom_point() + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Authors") +
  geom_abline(slope = 5, intercept = 25) +
  geom_abline(slope = 5, intercept = 22, lty = "dashed") +
  geom_abline(slope = 8, intercept = 30, lty = "dotted")
```

. . .

I'm just drawing any line that separates the two groups. Is there a better way to construct these lines?


## Equidistant Between Observations

. . .

::: columns
::: {.column width="47%"}
```{r}
#| label: margin-option-1
#| echo: false

fed_pca_jj %>%
  ggplot(aes(x = PC1, y = PC2, color = author_jj)) +
  geom_point() +
  geom_abline(slope = 5, intercept = 23.25) +
  geom_abline(slope = 5, intercept = 20, lty = "dashed") +
  geom_abline(slope = 5, intercept = 26.5, lty = "dashed") +
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Author")
```

:::

::: {.column width="3%"}
:::

::: {.column width="48%"}
::: {.fragment}
```{r}
#| label: margin-option-2
#| echo: false

fed_pca_jj %>%
  ggplot(aes(x = PC1, y = PC2, color = author_jj)) +
  geom_point() +
  geom_abline(slope = -8, intercept = -31.5) +
  geom_abline(slope = -8, intercept = -38, lty = "dashed") +
  geom_abline(slope = -8, intercept = -25, lty = "dashed") +
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Author")
```
:::
:::
:::

. . .

How should we choose between these two lines???

## Maximal Margin

The "best" line is the line with the [largest margin]{style="color: #b76352;"} or is furthest from the nearest observation on either side.

. . .

::: columns
::: {.column width="55%"}
```{r}
fed_pca_jj %>%
  ggplot(aes(x = PC1, y = PC2, color = author_jj)) +
  geom_point() +
  geom_abline(slope = -8, intercept = -31.5) +
  geom_abline(slope = -8, intercept = -38, lty = "dashed") +
  geom_abline(slope = -8, intercept = -25, lty = "dashed") +
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Author")
```
:::

::: {.column width="5%"}
:::

::: {.column width="40%"}
::: {.smaller}
::: {.incremental}
- Where could I add a John Jay essay which *would not* change the line?
- Where could I add a John Jay essay which *would* change the line?
- How many observations control the shape of the line?
:::
:::
:::
:::


## Difficulties with Hard Margins

</br>

It is rare to have observations that *perfectly* fall on either side of a line / hyperplane.

. . .

</br>

Adding **one more observation** could *totally change* our classification line!


## A Different Comparison

Suppose we wanted instead to separate "Hamilton" from "Not Hamilton"...

```{r}
fed_pca_ah %>% 
  ggplot(aes(x = PC1, y = PC2, color = author_ah)) +
  geom_point() + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Authors")
```

. . .

Where should we draw our line?

## Soft Margin

A [soft margin]{style="color: #b76352;"} is a margin with only a certain number of misclassified points.

. . .

There are two decisions to make here:

</br>

. . .

::: columns
::: {.column width="40%"}
1.  How big is our margin?

(`M` = width of margin)

:::

::: {.column width="5%"}
:::

::: {.column width="50%"}
::: {.fragment}
2.  How many misclassified observations are we willing to have?

(`C` = cost of a misclassified point)
:::
:::
:::

## An Initial Attempt

Width of margin: 2 

```{r}
fed_pca_ah %>%
  ggplot(aes(x = PC1, y = PC2, color = author_ah)) +
  geom_point() +
  geom_abline(slope = 2, intercept = 2) +
  geom_abline(slope = 2, intercept = 0, lty = 2) +
  geom_abline(slope = 2, intercept = 4, lty = 2) + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Authors")
```

. . .

How many points in the margin are misclassified?

## A Second Attempt

Width of margin: 1 

```{r}
fed_pca_ah %>% 
  ggplot(aes(x = PC1, y = PC2, color = author_ah)) +
  geom_point() +
  geom_abline(slope = 2, intercept = 2) + 
  geom_abline(slope = 2, intercept = 3, lty = "dashed") + 
  geom_abline(slope = 2, intercept = 1, lty = "dashed") + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Known Authors")
  
```

. . .

How many points in the margin are misclassified?


## Support Vector Classifier

The [support vector]{style="color: #ed8402;"} is the set of all observations that falling within the *soft margin* that are *misclassified*.

. . .

</br>

A [support vector classifier]{style="color: pink;"} tries to find:

. . .

a *line / hyperplane* that will be used to classify future observations ...

. . .

... that give us the biggest *margin width* (`M`)...

. . .

... while still respecting the cost of missclasified points (`C`).

## Fitting a Linear Support Vector Classifier with `tidymodels`

</br>

```{r}
#| label: linear-svm-specification-code
#| echo: true
#| eval: false
#| message: false

svm_spec <- svm_linear(cost = 3, margin = 0.5) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

fed_recipe <- recipe(author_ah ~ PC1 + PC2, data = fed_pca_ah)

fed_wflow <- workflow() %>%
  add_model(svm_spec) %>%
  add_recipe(fed_recipe)

my_svm <- fed_wflow %>%
  fit(fed_pca_ah)
```

::: {.callout-tip}
# Install a New Package

You will need to install the `kernlab` package this week!
:::

## Inspecting the Model Fit

```{r}
#| label: linear-svm-specification-fit
#| include: false

svm_spec <- svm_linear(cost = 3, margin = 0.5) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

fed_recipe <- recipe(author_ah ~ PC1 + PC2, data = fed_pca_ah)

fed_wflow <- workflow() %>%
  add_model(svm_spec) %>%
  add_recipe(fed_recipe)

my_svm <- fed_wflow %>%
  fit(fed_pca_ah)
```

```{r}
#| echo: true
#| label: svm-workflow-fit

my_svm %>% 
  extract_fit_parsnip()
```

## Making Predictions

```{r}
#| label: svm-prediction
#| echo: true

predict(my_svm, new_data = fed_pca_df)
```

## Plotting Predictions

```{r}
#| echo: false
#| label: svm-plotting-predictions

fed_pca_ah %>%
  mutate(
    preds = predict(my_svm, fed_pca_ah)$.pred_class, 
    author = if_else(author == "AH", 
                     "Hamilton", 
                     "Not Hamilton"
                     )
    ) %>%
  ggplot(aes(x = PC1, y = PC2, color = preds, pch = author)) +
  geom_point() + 
  labs(x = "Principle Component 1", 
       y = "Principle Component 2", 
       color = "Predicted Author", 
       pch = "Known Author")
```

. . .

How well did the model do?


## Model Metrics

```{r}
#| label: prediction-metrics
#| echo: true

predict(my_svm, new_data = fed_pca_ah) %>% 
  bind_cols(truth = fed_pca_ah$author_ah) %>% 
  rename(prediction = .pred_class) %>% 
  conf_mat(truth = truth, 
           estimate = prediction)
```

## Not Separable Groups

What if we simply couldn't separate our data with a line / hyperplane?

. . .

::: {.centered}
![](images/svm.png)
:::

## Increasing the Dimensions

What if we imagine our points exist in **three dimensions**?

. . .

:::{.centered}
![](https://codingmachinelearning.files.wordpress.com/2016/08/capture2.jpg?w=352&h=250)
:::

## Support Vector Machine

A [support vector machine]{style="color: #ed8402;"} is an extension of the support vector machine classifer that results from enlarging the feature space in a specific way, using [kernels]{style="color: #ed8402;"}. 

. . .

In this class, we will only implement *polynomial* kernals. 

## Try it!

Open [`Activity-SVM.qmd`](../../Classwork/Activity-SVM-2.qmd)

1.  Fit a support vector classifier, tuning the **cost** parameter

2.  Fit a support vector **machine**, tuning the **cost** parameter AND the **degree** parameter.
