---
title: "Principal Components Analysis"
format: 
  revealjs:
    theme: ../../../style.scss
editor: source
embed-resources: true
---

```{r}
#| label: setup
#| message: false
#| warning: false

options(htmltools.dir.version = FALSE, digits = 4, scipen = 999)

library(tidyverse)
library(tidymodels)
library(kknn)
library(glmnet)
library(discrim)
```

## Dimensionality

::: columns
::: {.column width="40%"}
Let's start thinking about our data in terms of *dimensions in space*.

::: fragment
Each **predictor** is an **axis**.
:::

::: fragment
The values of the predictors for a certain **observation** define a point in space.
:::
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
::: {.fragment}
When we compute distances between observations for KNN, we are computing **distance in space**.
:::

::: fragment
When we fit a **regression**, we are drawing a **straight line** through the points.
:::
:::
:::

## The Curse of Dimensionality

We run into trouble when we have **too many dimensions**. But what does "too many" mean?

::: columns
::: {.column width="33%"}
**Parametric** estimation -- We can't estimate 7000 coefficients from only 44 observations!
:::

::: {.column width="33%"}
::: {.fragment}
**Interpretability** -- Do we really want to translate our model into meaning for thousands of predictors???
:::
:::

::: {.column width="33%"}
::: {.fragment}
**Flexibility** -- More predictors = more flexibility = overfitting?
:::
:::
:::

## Principal Components Analysis

**PCA** is a way to *transform our data* (prior to modeling!) so that it has fewer **dimensions in space**.

. . .

::: columns
::: {.column width="30%"}
Instead of:

axis 1 = Predictor A

axis 2 = Predictor B

axis 3 = Predictor C
:::

::: {.column width="5%"}
:::

::: {.column width="65%"}
axis 1 = 0.5 (Pred A) + 0.2 (Pred B) + 0.3 (Pred C)

axis 2 = 0.1 (Pred A) + 0.7 (Pred B) + 0.2 (Pred C)

axis 3 = 0.1 (Pred A) + 0.2 (Pred B) + 0.8 (Pred C)
:::
:::

## PCA

![](https://miro.medium.com/freeze/max/499/1*V9yJUH9tVrMQI88TuIkCFQ.gif)

## PCA -- In 5 Steps!

1.  **Standardize** all axes

. . .

2.  Find the axis of **highest variance**: This is PC 1

. . .

3.  Find the axis of **highest variance** that is **perpendicular to PC 1**: This is PC2

. . .

4.  Continue until you have $p$ PCs, where $p$ = number of predictors (or, if $p > n$, until you have $n$ PCs).

. . .

5.  Use only the first $k$ predictors in your analysis, where $k < p$ and $k < n$.

## A New Dataset!

The **Federalist papers** are a series of essays written by John Jay, Alexander Hamilton, and James Madison.

Data: Proportion of the time each word was used in each essay (for the most common 200 words only).

. . .

```{r}
#| label: data-read-clean

# Read data
fed <- read.csv(here::here("06-Dimension-Reduction", 
                           "Slides", 
                           "PCA", 
                           "data", 
                           "federalist.txt")
                ) %>% 
  select(-X) 

# Keep numeric section only
fed_clean <- fed %>% 
  mutate(across(.cols = -Author, .fns = ~ as.numeric(.x)
                )
         )

auths <- fed$Author

# Data from papers with known authors
fed_known <- fed_clean %>% 
  filter(Author != "DIS")

auths_known <- fed_known$Author

# Data from papers with unknown authors
fed_unknown <- fed_clean %>% 
  filter(Author == "DIS") 
  
glimpse(fed_clean)
```

## Authors and Word Selection

If we choose a couple words and plot our data...

. . .

```{r}
#| label: two-words-plot

fed_known %>%
  ggplot(aes(x = there, y = would, color = auths_known)) +
  geom_point() +
  labs(x = "There", 
       y = "Would", 
       color = "Author", 
       title = "Proportion of Federalist Paper Essays Including a Given Word")
```

## Let's Give PCA a Try Instead!

. . .

```{r}
#| echo: true
#| label: pca-federalist

fed_matrix <- fed_known %>% 
  select(-Author)

pc <- prcomp(fed_matrix, 
             center = TRUE, 
             scale = TRUE)
```

## Previewing the Principal Components

```{r}
#| label: pc-variable-loadings
pc$rotation
```

## What variables matter most?

```{r}
#| label: pc-variable-importance

pcs_df <- pc$rotation %>%
  as.data.frame() %>%
  rownames_to_column() 

pcs_df %>%
  select(PC1:PC5) %>% 
  arrange(
    desc(
      abs(
        PC1
        )
      )
    )
```

##  {background-color="#B6CADA"}

::: {.centered}
::: {.larger}
How can we use our principal components to help us visualize the data?
:::
:::

## Locations of Observations on New (PC) Axes

```{r, echo = FALSE}
#| label: pc-locations

new_dims_df <- pc$x %>%
  as.data.frame() %>% 
  bind_cols(auths_known) %>% 
  rename(Author = `...71`)

new_dims_df %>% 
  select(PC1:PC5)
```

## Plotting the First Two Principal Components

```{r}
new_dims_df %>%
  ggplot(aes(x = PC1, y = PC2, color = Author)) +
  geom_point() + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2")
```

## Standard Deviations of PC Scores

```{r}
#| label: pc-loadings-stdev

pc$sdev
```

. . .

What do you notice???

## Cumulative Variances

```{r}
cumul_vars <- cumsum(pc$sdev^2) / sum(pc$sdev^2)
cumul_vars
```

. . .

How many principal components do we need to explain 85% of the variability in the data?

## A Cumulative Variance Plot

```{r}
tibble(cum_sum = cumul_vars, 
       PC = 1:70) %>% 
  ggplot(mapping = aes(x = PC, y = cum_sum)) +
  geom_point(alpha = 0.85, cex = 0.8) +
  labs(x = "Number of Principal Components Included", 
       y = "") +
  scale_y_continuous(labels = scales::percent)
  
```

## Some General Questions about Using PCA

::: columns
::: {.column width="45%"}
**How many PCs should we use?**

::: fragment
No single answer; people often do "enough for 90% variance covered" or similar.
:::

:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
::: {.fragment}
**How do you do modeling with PCA?**
:::

::: fragment
You don't. It's a data *preprocessing* step. You would then use `PC1`, `PC2`, etc **instead of** your original predictors.
:::

::: {.fragment}
Or you might use the variable importance measures to help decide which predictors to keep.
:::

:::
:::


## Perks of PCA 

::: columns
::: {.column width="45%"}
**Pros:**

-   Reduces dimension while still letting all original predictors be "involved"

-   Computationally fast for big data

-   Axis rotations are interpretable!

-   Dropping off lower PCs gets rid of noise (maybe)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
**Cons:**

-   Using PCs in interpretable models makes them uninterpretable -- what does the coefficient of PC1 mean in real life?

-   No magic answer for how many PCs to use.

-   Dropping off lower PCs gets rid of useful info (maybe)
:::
:::

## But wait - isn't this LDA?

Recall that **LDA** also found *scores* for your observations based on a *linear combination* of the original predictors.

. . .

So what's the difference?

. . .

::: columns
::: {.column width="40%"}
LDA loadings are trying to maximize the **difference in mean scores across categories**.

This is a *supervised* method!
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
::: fragment
PCA loadings are trying to maximize the **variance of the scores** on the PC axes.

This is an *unsupervised* method!
:::
:::
:::

## Try it!

Download and open [`Activity-PCA.qmd`](../../Classwork/Activity-PCA.qmd)

1.  Apply PCA to the cannabis data

2.  Interpret the PC rotations

3.  Plot the data on the first two axes, and color by Type.

4.  Choose a "good" number of PCs to use.

5.  Fit a KNN classifier using:

- All the variables
- Only the 5 most important variables according to PCA
- Only your chosen PCs

6.  Compare the accuracy of these three approaches!
