---
title: "Clustering"
format: 
  revealjs:
    theme: [../../../style.scss, dark]
editor: visual
embed-resources: true
---

```{r}
#| label: setup
#| include: false

options(htmltools.dir.version = FALSE, 
        digits = 4, 
        scipen = 999)

library(tidyverse)
library(tidymodels)
library(kknn)
library(glmnet)
library(discrim)
```

## Unsupervised Learning

So far in this class, we've only done [supervised learning]{style="color: #ed8402;"}. Meaning, we have a [response variable]{style="color: #ed8402;"} and we observe its value for all or some of our observations.

. . .

</br>

Clustering is a type of [unsupervised learning]{style="color: #b87579;"}. Meaning, we want to sort our observations into clusters based on the predictors, but we don't have a pre-conceived notion of [what those clusters represent]{style="color: #b87579;"}!

## Clustering

The general goal of clustering is to make clusters such that points **within** a cluster are closer to each other than to the points **outside** the cluster.

. . .

</br>

::: columns
::: {.column width="30%"}
What is our definition of *close*?
:::

::: {.column width="30%"}
How *many* clusters do we think exist?
:::

::: {.column width="30%"}
What *algorithm* do we use to select the clusters?
:::
:::


## Clustering Usage 

::: {.centered}
![](images/clustering-articles.png)   
:::

## K-means Clustering Algorithm

Idea:  Iteratively update the **centers** of the clusters until convergence.

. . .

1. Plop $k$ points down in space. These are the *centroids*.

::: {.callout-tip}
# Choosing Initial Centroids

There are many ways to choose initial centroids -- randomly in space, randomly chosen observations in the dataset, the farthest point(s). 
:::
. . .

2. Determine which centroid each observation is closest to.  Assign it to that cluster.


## K-means Clustering Algorithm Continued


3. Find the mean of each cluster.  These are the *new* centroids.

. . .

4. Continue this process (assigning points to clusters and updating the centroids) until the centroids don't change.

## A K-Means Annimation

![](images/annimation.png)

</br> 

<https://www.naftaliharris.com/blog/visualizing-k-means-clustering/>


## K-means Clustering in R

The `kmeans()` function must be given a numeric [matrix / dataframe]{style="color: #76b5c5;"} and a [number of clusters]{style="color: #b76352;"}.

It gives back centroids, cluster assignments, and sum-of-squares.

. . .

```{r, echo = FALSE}
#| label: data-load-clean
#| echo: false
#| message: false

# Read data
fed <- read.csv("https://www.dropbox.com/s/9t8sxr1sg0monih/federalist.txt?dl=1")
fed <- fed[,-1]

# Keep numeric section only
fed_all <- as.matrix(fed[,-1])
fed_all <- apply(fed_all, 2, as.numeric)

auths <- fed$Author

# Data from papers with known authors
fed_known <- as.matrix(fed[auths != 'DIS',-1])
fed_known <- apply(fed_known, 2, as.numeric)

auths_known = auths[auths != 'DIS']

# Data from papers with unknown authors
fed_unknown <- as.matrix(fed[auths == 'DIS',-1])
fed_unknown <- apply(fed_unknown, 2, as.numeric)

fed_ex <- as_tibble(fed_known) %>% 
  cbind(auths_known)
fed_matrix <- fed_ex %>% 
  select(-auths_known) %>% 
  as.matrix()
```

. . .

</br>

```{r}
#| label: kmeans-fit
#| echo: true

fed_km <- kmeans(fed_matrix, 
                 centers = 3)
```

::: {.callout-tip}
# Optional Arguments

`nstart`, the number of random sets that should be chosen, is by default set to 1

`algorithm` -- allows you to choose which K-means algorithm you would like to use
:::

## Centers

```{r}
#| label: kmeans-centers
#| echo: true

fed_km$centers
```

## Sums of Squares

```{r}
#| label: kmeans-ss
#| echo: true

fed_km$totss
fed_km$withinss
fed_km$betweenss
```

## Cluster Assignments

::: columns
::: {.column width="50%"}
```{r}
#| label: kmeans-cluster-assignments
#| echo: true

res <- tibble(
  clust = fed_km$cluster, 
  auth = fed_ex$auths)

res
```
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

::: {.fragment}
```{r}
res %>% 
  count(clust, auth)
```
:::

:::
:::


## K-means + PCA

::: columns
::: {.column width="45%"}
Did we really need all 200 variables to find those clusters?  
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
Did we maybe "muddy the waters" by weighing all variables equally?
:::
:::

. . .

</br>

It is **very common** to do a PCA reduction before running k-means!

## Preprocessing with PCA

```{r}
#| echo: true
#| label: pca-kmeans-3-centers
pc <- prcomp(fed_matrix, 
             center = TRUE, 
             scale = TRUE)

fed_reduced <- pc$x[, 1:2]

fed_pca_km <- kmeans(fed_reduced, 
                     centers = 3)
```

. . .

```{r}
#| label: pca-kmeans-3-centers-results
#| echo: true
res <- tibble(
  clust = fed_pca_km$cluster, 
  auth = fed_ex$auths)

res %>% 
  count(clust, auth)
```


## What if we'd done four centroids?

```{r}
#| echo: false
#| label: pca-kmeans-4-centers

pc <- prcomp(fed_matrix, 
             center = TRUE, 
             scale = TRUE)

fed_reduced <- pc$x[, 1:2]

fed_pca_km <- kmeans(fed_reduced, 
                     centers = 4)

res <- tibble(
  clust = fed_pca_km$cluster, 
  auth = fed_ex$auths)

res %>% 
  count(clust, auth)
```


## K-means Takeaways


::: columns
::: {.column width="45%"}
**Pros:**

* Simple algorithm, easy to understand

* Plays nice with PCA

* SUPER fast to compute
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
**Cons:**

* Very sensitive to location of initial centroids

* User has to pick the number of clusters
:::
:::

## Try it!

Open a **Activity-Clustering.Rmd**

1. Apply k-means clustering to the cannabis data using *all* the word predictors.
2. What was the within and between sum of squares?
3. Did the clusters match up with the Type?

Now, refer back to your PCA analysis of the cannabis data (from Monday).

4. Apply k-means clustering to the **second and third** PC only
5. Plot these clusters.  What do you think they capture?


## Hierarchical Clustering

(also called **agglomerative** clustering)

Idea:  Merge observations that are close together.

. . .

1. Find the closest two observations.  Replace them with their centroid.

. . .

2. Find the next two closest observations.  (One might be the centroid!) Replace them with their centroid.

. . .

3. Continue until all observations have been merged.


##  {background-color="#B6CADA"}


::: columns
::: {.column width="45%"}
![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png)
:::
:::


## Hierarchical clustering

The `hclust()` function needs to be given a *distance matrix*.

. . .

```{r}
#| error: true
#| echo: true

fed_hc <- fed_matrix %>% 
  hclust()
```


## Hierarchical clustering

When you give `hclust()` a distance matrix...

```{r}
#| echo: true
fed_hc <- fed_matrix %>% 
  dist() %>% 
  hclust()
```

. . .

it gives back a **dendrogram**.

```{r}
#| echo: true
plot(fed_hc, 
     labels = fed_ex$auths)
```


## Choosing Clusters from Dendrograms

To decide how to assign clusters, we can choose **how many** clusters we want...

```{r}
#| echo: true
res_hc <- cutree(fed_hc, 
                 k = 3)

res_hc
```

```{r}
#| echo: true

tibble(
  clust = res_hc,
  auth = fed_ex$auths
) %>%
  count(clust, auth)
```


## Option 2 

Or we can choose a **height cutoff** for the dendrogram

```{r}
#| echo: true

res_hc_2 <- cutree(fed_hc, 
                   h = 0.05)
res_hc_2
```

. . .

```{r}
#| echo: true

tibble(
  clust = res_hc_2,
  auth = fed_ex$auths
) %>%
  count(clust, auth)
```


## Hierarchical Clustering Takeaways

::: columns
::: {.column width="40%"}
**Pros:**

* Fast computation for moderate sized data

* Gives back full information in dendrogram form

:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
**Cons:**

* User has to decide how to go from dendrogram to cluster assignments

:::
:::

## Try it!

Open `Activity-Clustering.qmd`

1 Apply hierarchical clustering to the cannabis data
2. Compare your results to k-means. 
3. Which method do you prefer?  Why?
