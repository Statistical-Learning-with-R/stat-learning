---
title: "Clustering"
format: 
  revealjs:
    theme: [../../../style.scss, dark]
editor: source
embed-resources: true
---

```{r}
#| label: setup
#| include: false

options(htmltools.dir.version = FALSE, 
        digits = 4, 
        scipen = 999)

library(tidyverse)
library(tidymodels)
library(kknn)
library(glmnet)
library(discrim)
```

## Unsupervised Learning

So far in this class, we've only done [supervised learning]{style="color: #ed8402;"}. Meaning, we have a [response variable]{style="color: #ed8402;"} and we observe its value for all or some of our observations.

. . .

</br>

Clustering is a type of [unsupervised learning]{style="color: #b87579;"}. Meaning, we want to sort our observations into clusters based on the predictors, but we don't have a pre-conceived notion of [what those clusters represent]{style="color: #b87579;"}!

## Clustering Usage

::: centered
![](images/clustering-articles.png)
:::

## Clustering

The general goal of clustering is to make clusters such that points **within** a cluster are closer to each other than to the points **outside** the cluster.

. . .

</br>

::: columns
::: {.column width="30%"}
What is our definition of *close*?
:::

::: {.column width="30%"}
How *many* clusters do we think exist?
:::

::: {.column width="30%"}
What *algorithm* do we use to select the clusters?
:::
:::

## K-means Clustering Algorithm

Idea: Iteratively update the **centers** of the clusters until convergence.

. . .

1.  Plop $k$ points down in space. These are the *centroids*.

::: callout-tip
# There are many ways to choose initial centroids!
:::

. . .

2.  Determine which centroid each observation is closest to. Assign it to that cluster.

## K-means Clustering Algorithm Continued

3.  Find the mean of each cluster. These are the *new* centroids.

. . .

4.  Continue this process (assigning points to clusters and updating the centroids) until the centroids don't change.

## A K-Means Annimation

![](images/annimation.png)

</br>

<https://www.naftaliharris.com/blog/visualizing-k-means-clustering/>

## K-means Clustering in R

The `kmeans()` function must be given a numeric [matrix / dataframe]{style="color: #76b5c5;"} and a [number of clusters]{style="color: #b76352;"}.

It gives back centroids, cluster assignments, and sum-of-squares.

. . .

```{r}
#| label: data-load-clean
#| echo: false
#| message: false

# Read data
fed <- read.csv(here::here("06-Dimension-Reduction", 
                           "Slides", 
                           "PCA", 
                           "data", 
                           "federalist.txt")
                ) %>% 
  select(-X) 

# Keep numeric section only
fed_clean <- fed %>% 
  mutate(across(.cols = -Author, .fns = ~ as.numeric(.x)
                )
         )

auths <- fed$Author

# Data from papers with known authors
fed_known <- fed_clean %>% 
  filter(Author != "DIS")

auths_known <- fed_known$Author

# Data from papers with unknown authors
fed_unknown <- fed_clean %>% 
  filter(Author == "DIS") 


fed_matrix <- fed_known %>% 
  select(-Author)
```

. . .

</br>

```{r}
#| label: kmeans-fit
#| echo: true

fed_km <- kmeans(fed_matrix, 
                 centers = 3)
```

::: callout-tip
# Optional Arguments

`nstart` -- the number of random sets that should be chosen, is by default set to 1

`algorithm` -- allows you to choose which K-means algorithm you would like to use (Loyd, MacQueen, or Hartigan-Wong)
:::

<!-- Hartigan and Wong's method starts by partitioning the points into random clusters -->

<!-- Loyd -- randomly assigns all points to a centroid -->

<!-- MacQueen -- randomly chooses k datapoints / observations as centroids -->

## Centers

```{r}
#| label: kmeans-centers
#| echo: true

fed_km$centers
```

## Sums of Squares

::: columns
::: {.column width="30%"}
Total Sum of Squares

```{r}
#| label: kmeans-total-ss
#| echo: true

fed_km$totss
```
:::

::: {.column width="30%"}
::: fragment
Within Sum of Squares

```{r}
#| label: kmeans-within-ss
#| echo: true

fed_km$withinss
```
:::
:::

::: {.column width="30%"}
::: fragment
Between Sum of Squares

```{r}
#| label: kmeans-between-ss
#| echo: true

fed_km$betweenss
```
:::
:::
:::

## Cluster Assignments

::: columns
::: {.column width="50%"}
```{r}
#| label: kmeans-cluster-assignments
#| echo: true

res <- tibble(
  clust = fed_km$cluster, 
  auth = fed_known$Author)

res
```
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
::: fragment
```{r}
#| echo: true
res %>% 
  count(clust, auth)
```
:::
:::
:::

## K-means + PCA

::: columns
::: {.column width="45%"}
Did we really need all 200 variables to find those clusters?
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
Did we maybe "muddy the waters" by weighing all variables equally?
:::
:::

. . .

</br>

It is **very common** to do a PCA reduction before running K-means!

## Integrating PCA with K-means

::: columns
::: {.column width="45%"}
**Pre-processing**

```{r}
#| echo: true
#| label: pca-kmeans-3-centers
pc <- prcomp(fed_matrix, 
             center = TRUE, 
             scale = TRUE)

fed_reduced <- pc$x[, 1:2] 
```
:::

::: {.column width="50%"}
::: fragment
**Fitting**

```{r}
#| echo: true
fed_pca_km <- kmeans(fed_reduced, 
                     centers = 3)
```
:::
:::
:::

. . .

**Inspecting**

```{r}
#| label: pca-kmeans-3-centers-results
#| echo: true
res3 <- tibble(
  clust = fed_pca_km$cluster, 
  auth = fed_known$Author)

res3 %>% 
  count(clust, auth)
```

## What if we'd done four centroids?

::: columns
::: {.column width="45%"}
**Three Centroids**

```{r}
res3 %>% 
  count(clust, auth)
```
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
**Four Centroids**

```{r}
#| echo: false
#| label: pca-kmeans-4-centers

pc <- prcomp(fed_matrix, 
             center = TRUE, 
             scale = TRUE)

fed_reduced <- pc$x[, 1:2]

fed_pca_km <- kmeans(fed_reduced, 
                     centers = 4)

res4 <- tibble(
  clust = fed_pca_km$cluster, 
  auth = fed_known$Author)

res4 %>% 
  count(clust, auth)
```
:::
:::

## K-means Takeaways

::: columns
::: {.column width="45%"}
**Pros:**

-   Simple algorithm, easy to understand

-   Plays nice with PCA

-   SUPER fast to compute
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
**Cons:**

-   Very sensitive to location of initial centroids

-   User has to pick the number of clusters
:::
:::

## Try it!

Open [`Activity-Clustering.qmd`](../../Classwork/Activity-Clustering.qmd)

1.  Apply k-means clustering to the cannabis data using *all* the word predictors.
2.  What was the within and between sum of squares?
3.  Did the clusters match up with the Type?

Now, refer back to your PCA analysis of the cannabis data (from Monday).

4.  Apply k-means clustering to the **second and third** PC only
5.  Plot these clusters. What do you think they capture?

## Hierarchical Clustering

(also called **agglomerative** clustering)

Idea: Merge observations that are close together.

. . .

1.  Find the closest two observations. Replace them with their centroid.

. . .

2.  Find the next two closest observations. (One might be the centroid!) Replace them with their centroid.

. . .

3.  Continue until all observations have been merged.

##  {background-color="#B6CADA"}

::: larger
Merging Observations
:::

::: columns
::: {.column width="45%"}
![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
![](images/hierarchical-final.png)
:::
:::

## Hierarchical Clustering in R

The `hclust()` function needs to be given a *distance matrix*.

. . .

</br>

```{r}
#| error: true
#| echo: true

fed_hc <- fed_matrix %>% 
  hclust()
```

## Hierarchical Clustering in R

When you give `hclust()` a distance matrix...

```{r}
#| echo: true
fed_hc <- fed_matrix %>% 
  dist() %>% 
  hclust()
```

## it gives back a (not super pretty) *dendrogram*.

```{r}
#| echo: true
plot(fed_hc, 
     labels = fed_known$Author)
```

## Choosing Clusters from Dendrograms

To decide how to assign clusters, we can choose *how many* clusters (`k`) we want we can use `cuttree()` to cut the dendrogram into a specific number of clusters.

```{r}
#| echo: true
res_hc <- cutree(fed_hc, 
                 k = 3)

res_hc
```

. . .

```{r}
#| echo: true
#| output-location: column

tibble(
  clust = res_hc,
  auth = fed_known$Author
) %>%
  count(clust, auth)
```

## Option 2

Or we can choose a *height cutoff* (`h`) for the dendrogram

```{r}
#| echo: true

res_hc_2 <- cutree(fed_hc, 
                   h = 0.05)
res_hc_2
```

. . .

```{r}
#| echo: true
#| output-location: column

tibble(
  clust = res_hc_2,
  auth = fed_known$Author
) %>%
  count(clust, auth)
```

## Hierarchical Clustering Takeaways

::: columns
::: {.column width="40%"}
**Pros:**

-   Fast computation for moderate sized data

-   Gives back full information in dendrogram form
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
**Cons:**

-   User has to decide how to go from dendrogram to cluster assignments
:::
:::

## Try it!

1.  Apply hierarchical clustering to the cannabis data
2.  Compare your results to k-means.
3.  Which method do you prefer? Why?
