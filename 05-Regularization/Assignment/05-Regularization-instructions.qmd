---
title: "Lab 5: Variable Selection and Regularization"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
#| label: libraries-r
#| include: false
library(tidyverse)
library(tidymodels)
library(glmnet)
library(discrim)
library(rpart)
library(rpart.plot)
library(baguette)
```

# Dataset: Gene Expression

Technology for measuring gene expression from human blood samples first became generally available to scientists in the 1990s. It was quickly found that this information was extremely powerful for predicting and categorizing diseases in humans, especially cancers.

At the time, it was very costly to process even a single sample. Thus, it was common to have datasets with large numbers of variables (genes) but very small numbers of samples (people). This data format went against all of classical statistics, which tends to assume many samples on a few variables. The field had to adapt accordingly, leading to methods like LASSO Regression.

Historical interlude: This area of study in statistics was called "High Dimension, Low Sample Size". Nowadays, the technology is much cheaper, and we often have very large sample sizes as well as very large variable sizes - another new problem for the early to mid 2000s that we called"Big Data". You can also read the original LASSO paper from 1996 here, if you're interested: https://statweb.stanford.edu/\~tibs/lasso/lasso.pdf. Fun fact: it's the second-most cited paper in all of statistics!

This lab's data is taken from a paper in 1999, which used clustering and classification techniques to classify patients between two types of cancer: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).

You can read the paper here (although you don't need to for this lab): https://webdocs.cs.ualberta.ca/\~rgreiner/R/OLD-BiCluster/Golub_et_al_1999.pdf

In this lab, we'll see if we can learn more about the genetic drivers of AML and ALL via modern statistical learning techniques.

# Template Quarto File & Datasets

Download the template Quarto file here: [`05-Regularization-template.qmd`](05-Regularization-template.qmd) 

Download the main dataset [(`genes_cancer_train.csv`)](data/genes_cancer_train.csv) and the validation dataset [(`genes_cancer_validate.csv`)](data/genes_cancer_validate.csv).

```{r}
#| label: data-load
#| message: false
#| echo: false

genes <- read_csv(here::here("05-Regularization", 
                             "Assignment", 
                             "data", 
                             "genes_cancer_train.csv")
                  ) 
genes_validation <- read_csv(here::here("05-Regularization", 
                             "Assignment", 
                             "data", 
                             "genes_cancer_validate.csv")
                  )
```

## Reducing Size of Data

For most of this lab (right up until the end), we will use a smaller version of the datasets, so as not to murder your personal computers. We'll choose a random 100 of the variables:

```{r}
#| label: subsetting-data

set.seed(282)

random_columns <- sample(3:ncol(genes), 
                         size = 100)

genes_sub <- genes %>% 
  select(1, 
         2, 
         all_of(random_columns)
         )
```

# Some Words of Caution

A few words of caution for this lab:

1.  Don't forget to clean and adjust your data. You'll need to drop the id column (`patient`), since we don't want to use the patient number as a predictor. There are two ways to do this:

- Remove `patient` from the datasets
- Change the role of the `patient` variable in a recipe step (code shown below)

```{r}
#| eval: false
#| label: example-update-role

recipe(MODEL, data = genes_sub) %>%
  update_role(patient, new_role = "id variable")
```

2.  There are a **lot** of variables in the dataset! You'll be using the `cancer ~ .` trick in your recipes and model formulas, rather than typing out a ton of gene names. Be careful what kind of models and functions you put this into; some might overwhelm your computer.

3.  In particular, be careful when using the `regsubsets()` function. Make sure you avoid the exhaustive (all subsets) search... You'd be asking your computer to fit more models than there are atoms in the universe! Make **absolutely** sure that you set the `method` to be either "forward" or "backward", and also that you choose a value for `nvmax`.

4.  Even with the above precautions, we are getting into Big Computation territory. Small errors in this lab have the potential to crash your R session. Save your Quarto file (and any other unsaved files on your computer frequently), and don't be afraid to force-quit RStudio if you accidentally run something bigger than you meant to. When in doubt, **start with small examples** and work your way up!

5.  Use `glmnet` as your engine whenever you are doing LASSO or Ridge of any kind.

# Part One: Classification without regularization

::: {.callout-important}
# Use your `genes_sub` dataset for everything in this part.
:::

#### Q1: Decision Tree

Fit a decision tree to this data. Which genes does it designate as most important for differentiating between ALL and AML cancers? How pure are the nodes?

#### Q2: Validation

Use your tree to predict on the validation set. How did it do?

#### Q3: Explanation

Give an intuitive explanation for why the above occurred.

That is to say: Your tree should have only one split, so it can't be too overfit. It should also have very good node purity, suggesting good predictive power. But the accuracy in Q2 is not as high as the node purity would suggest. Why is this?

#### Q4: Random Forest

Now fit a Random Forest to the data. 

#### Q5: Validation

Use your random forest to predict on the validation set. How did it do? 

#### Q6: Explanation

How does this method compare to a single decision tree? Give an explanation for the difference in results.

# Part Two: Variable Selection

#### Q7: Stepwise Selection

Use forwards or backwards selection (your choice) to choose the ideal number of variables, up to a maximum of 10. Which genes are chosen?

::: {.callout-caution}
# Changing Cancer to an Integer

You will have to convert `cancer` to a fake quantitative variable first. This is obviously not really justifiable, and it's not what we'd do if we were serious about stepwise selection - but we're just making a fast-and-dirty comparison here.

The code below will do the trick.

```{r}
#| label: cancer-to-integer

genes_weird_sub <- genes_sub %>%
  mutate(
    cancer = factor(cancer), 
    cancer = as.integer(cancer)
  )
```
:::

::: {.callout-tip}
# Grabbing Variables

To find the selected variables from a particular row of the "asterisks" output, use:

```{r}
#| label: code-to-grab-variables-selected
#| eval: false

vars <- summary(models)$outmat[1,]
names(vars[vars == "*"])
```
:::

#### Q8: Tuning LASSO

Tune a LASSO regression. Identify the largest `penalty` parameter that doesn't cause you to lose any prediction accuracy.

::: {.callout-tip}
# A Recommended Grid

Use the following lambda grid. Make sure you **look** at the range of values that it chooses, rather than just copy pasting the code. ðŸ˜‰

```{r}
#| label: lambda-grid

lam_grid <- grid_regular(penalty(c(-10, 0), 
                                 trans = log2_trans()), 
                         levels = 10)
```
:::

#### Q9: LASSO Variable Selection

Using the penalty chosen in Q2, fit a final LASSO model on the **full data** (that is, on `genes` not on `genes_sub`). What genes were selected?

# Part Three: Reducing Variance of Coefficients

#### Q10: Ordinary Logistic Regression

Randomly divide the observations in the dataset (in `genes_sub`) in half. Fit a logistic regression on the with no penalty term to each half.

Report the estimates for the first five listed predictors. How different were they between the two subsamples of the dataset?

#### Q11: Ridge regression - tuning.

Tune a logistic regression with ridge penalty on `genes_sub`. Once again, choose the largest penalty that does not noticeably decrease the accuracy.

#### Q12: Comparison

Fit a logistic regression with the penalty selected in Q11 on the two random halves of the dataset that you created in Q10.

Compare the estimates of the first five variables. How different were they?

#### Q13: Explanation

In your own words, give an explanation for what we saw in Q10 versus in Q12. Why were the results similar or different?

# Part Four: A final model

#### Q14: Tuning

Using `genes_sub`, tune both the penalty and the mixture term. Choose the penalty that is largest without losing accuracy.

::: {.callout-tip}
# `levels` in Tuning Grid

Remember that the `levels` argument in your `grid_regular` function is telling you the number of unique values of **each** parameter that will be tried!
:::

#### Q15: Mixture Parameter

Interpret the selected `mixture` parameter in words.

#### Q16: Conclusion

Using the parameters you selected above, fit your model to the **full dataset**.

How many genes were selected? Which seem to be most important?

Report the performance of the model on the validation set.
