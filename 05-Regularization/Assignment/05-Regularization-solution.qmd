---
title: "Assignment 5: Regularization"
author: "Solutions by Dr. Theobold"
format: html
embed-resources: true
editor: source
execute: 
  echo: fenced
---

Loading packages and setting a seed: 

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(leaps)
library(rpart.plot)

set.seed(282)
```

Load the main dataset, and the validation set:

```{r}
#| label: data-load-clean
#| message: false
#| warning: false

genes <- read_csv(here::here("05-Regularization", 
                             "Assignment", 
                             "data", 
                             "genes_cancer_train.csv")
                  ) 
genes_validation <- read_csv(here::here("05-Regularization", 
                             "Assignment", 
                             "data", 
                             "genes_cancer_validate.csv")
                  )

## Convert response to factor since that is what tidymodels expects
genes <- genes %>%
  mutate(
    cancer = factor(cancer)
  )

genes_validation <- genes_validation %>%
  mutate(
    cancer = factor(cancer)
  )
```

## Subsetting Data

For most of this lab (right up until the end), we will use a smaller version of the datasets, so as not to murder your personal computers. We'll choose a random 100 of the variables:

```{r}
#| label: subsetting-data

random_columns <- sample(3:ncol(genes), 
                         size = 100)

genes_sub <- genes %>% 
  select(1, 
         2, 
         all_of(random_columns)
         )
```

# Part One: Classification Without Regularization

```{r}
#| label: genes-recipes

genes_rec_sub <- recipe(cancer ~ ., data = genes_sub) %>%
  update_role(patient, new_role = "id variable")

genes_rec_full <- recipe(cancer ~ ., data = genes) %>%
  update_role(patient, new_role = "id variable")

```

#### Q1: Decision Tree

Fit a decision tree to this data. Which genes does it designate as most important for differentiating ALL and AML cancers? How pure are the nodes?

```{r}
#| label: decision-tree

tree_spec <- decision_tree() %>%
  set_mode("classification") %>%
  set_engine("rpart")

tree_fit <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(genes_rec_sub) %>%
  fit(genes_sub)

tree_fitted <- tree_fit %>% 
  extract_fit_parsnip() 

rpart.plot(tree_fitted$fit, roundint = FALSE)
```

*Based on the decision tree, `M96326_rna1_at` is the main gene that is designated as important for differentiating ALL from AML cancer. The resulting leaves are almost entirely pure, with only 9% of observations in the ALL leaf being misclassified.*

#### Q2: Validation

Use your tree to predict on the validation set. How did it do?

```{r}
#| label: tree-validation

genes_validation %>%
  mutate(
    preds = predict(tree_fit, genes_validation)$.pred_class
  ) %>%
  accuracy(truth = cancer, estimate = preds)

```

It did fairly well, correctly classifying the type of cancer 85% of the time. This is about 20% more effective than just guessing ALL every time (since it occurs in 65% of the observations). 

```{r}
#| label: null-classifier-comparison
genes_sub %>% 
  count(cancer) %>% 
  mutate(prop = n / sum(n))
```


#### Q3: Explanation.

The tree had 1000 variables to choose from. (If we weren't limiting ourselves, it'd be over 7000!) In the smaller, subset dataset there were only 44 samples. It is fairly easy to find some variable among the 1000 that has a cutoff that nearly perfectly splits the 44 observations into their groups, mostly by luck.

When we predict on the validation set, we do *okay*, because the variable is probably somewhat meaningful - but not as perfectly as we might expect, and not as well as we probably could have done with more variables involved.

This is a form of overfitting, even though it doesn't appear so from the tree. When the model has too many variables to choose from, it can have "too much success" on the training data.

#### Q4: Random Forest

Now fit a Random Forest to the data.

```{r}
#| label: random-forest

rf_spec <- rand_forest() %>%
  set_mode("classification") %>%
  set_engine("ranger")

rf_fit <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(genes_rec_sub) %>%
  fit(genes_sub)

```

#### Q5: Validation

Use your random forest to predict on the validation set. How did it do?

```{r}
genes_validation %>%
  mutate(
    cancer = factor(cancer),
    preds = predict(rf_fit, genes_validation)$.pred_class
  ) %>%
  accuracy(truth = cancer, estimate = preds)
```

This is pretty good! Our accuracy improved by 10% from the decision tree and we are now doing over 30% better than the null classifier. 

#### Q6: Explanation

How does this method compare to a single decision tree? Give an explanation for the difference in results.

The random forest gains about 5% in accuracy over the decision tree, which is driven by the random forest allowing other variables to contribute to the decision tree. 

# Part Two: Variable Selection

```{r}
#| label: cancer-to-integer

genes_weird_sub <- genes_sub %>%
  mutate(
    cancer = as.integer(
      factor(cancer)
      )
  )
```

#### Q7: Stepwise selection.

Use forwards or backwards selection (your choice) to choose the ideal number of variables, up to a maximum of 10. Which genes are chosen?

```{r}
#| label: backward-selection
#| cache: true
#| warning: false

models <- regsubsets(cancer ~ ., data = genes_weird_sub, 
                     method = "backward", 
                     nvmax = 10)

bic_scores <- summary(models)$bic
best_model <- which.min(bic_scores)

vars <- summary(models)$outmat[best_model, ]
best_vars <- names(vars[vars == "*"])
best_vars
```

#### Q8: LASSO.

Tune a LASSO regression. Identify the largest penalty parameter that doesn't cause you to lose any prediction accuracy.

```{r}
#| label: lambda-grid

lam_grid <- grid_regular(penalty(c(-10, 0), 
                                 trans = log2_trans()
                                 ), 
                         levels = 10)
```

```{r}
#| label: lasso-tune
#| cache: true

lass_spec <- logistic_reg(penalty = tune(), 
                          mixture = 1
                          ) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

cvs <- vfold_cv(genes_sub, v = 5)

lass_wflow <- workflow() %>%
  add_recipe(genes_rec_sub) %>%
  add_model(lass_spec)

tuning <- lass_wflow %>%
  tune_grid(
    resamples = cvs,
    grid = lam_grid
  )

tuning %>% 
  show_best(metric = "accuracy")
```

The largest penalty with the best accuracy is 0.0992, however lower penalties of 0.0213 and 0.0459 have accuracies that are less than 1% away from the top.

#### Q9: LASSO Variable Selection.

Using the penalty chosen in Q2, fit a final LASSO model on the **full data** (that is, on `genes` not on `genes_sub`). What genes were selected?

```{r}
#| label: lasso-variable-selection
#| cache: true

lass_spec_untune <- logistic_reg(penalty = 0.0992, 
                                 mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

workflow() %>%
  add_recipe(genes_rec_full) %>%
  add_model(lass_spec_untune) %>%
  fit(genes) %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  filter(estimate != 0) %>% 
  select(term)
```

# Part Three: Reducing Variance of Coefficients

#### Q10: Ordinary Logistic Regression

Randomly divide the observations in the dataset (back to `genes_sub`) in half. Fit a logistic regression on the with no penalty term to each half.

Report the estimates for the first five predictors (not the Intercept!). How different were they between the two subsamples of the datset?

```{r}
#| label: logistic-regression
#| cache: true

splits <- initial_split(genes_sub, 0.5)
genes_sub_1 <- training(splits)
genes_sub_2 <- testing(splits)

logistic_nopenalty_spec <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm")
```

::: columns
::: {.column width="47%"}
**Split 1**

```{r}
#| label: split1-logistic-results

workflow() %>%
  add_recipe(genes_rec_sub) %>%
  add_model(logistic_nopenalty_spec) %>%
  fit(genes_sub_1) %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  select(term, estimate) %>%
  filter(term != "(Intercept)") %>%
  slice_head(n = 5)

```

:::

::: {.column width="5%"}
:::

::: {.column width="48%"}

**Split 2**

```{r}
#| label: split2-logistic-results

workflow() %>%
  add_recipe(genes_rec_sub) %>%
  add_model(logistic_nopenalty_spec) %>%
  fit(genes_sub_2) %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  select(term, estimate) %>%
  filter(term != "(Intercept)") %>% 
  slice_head(n = 5)
```

:::
:::

</br>

Very different! `M55543_at` was a top predictor for both splits, but all of the other top predictors are different between the two splits. 


#### Q11: Ridge Regression - Tuning

Tune a logistic regression with ridge penalty on `genes_sub`. Once again, choose the largest penalty that does not noticeably decrease the accuracy.

```{r}
#| label: ridge-regression-tune
#| cache: true

ridge_spec <- logistic_reg(penalty = tune(),
                           mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

ridge_wflow <- workflow() %>%
  add_recipe(genes_rec_sub) %>%
  add_model(ridge_spec)

tuning <- ridge_wflow %>%
  tune_grid(
    resamples = cvs,
    grid = lam_grid
  )

tuning %>% 
  show_best(metric = "accuracy")
```

For ridge regression, a penalty of 0.0213 is the largest with the same predictive accuracy.

#### Q12: Comparison

Fit a logistic regression with the penalty selected in Q11 on the two random halves of the dataset that you created in Q10.

Compare the estimates of the top five predictors (not the Intercept!). How different were they?

```{r}
#| label: ridge-comparison
#| cache: true

ridge_spec_untune <- logistic_reg(penalty = 0.0213, 
                                  mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```


::: columns
::: {.column width="47%"}
**Split 1**

```{r}
#| label: split1-ridge-results
#| message: false
#| warning: false

workflow() %>%
  add_recipe(genes_rec_sub) %>%
  add_model(ridge_spec_untune) %>%
  fit(genes_sub_1) %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  select(term, estimate) %>%
  filter(term != "(Intercept)") %>% 
  slice_max(order_by = estimate, n = 5)
```

:::

::: {.column width="5%"}
:::

::: {.column width="48%"}
**Split 2**

```{r}
#| label: split2-ridge-results
#| message: false
#| warning: false

workflow() %>%
  add_recipe(genes_rec_sub) %>%
  add_model(ridge_spec_untune) %>%
  fit(genes_sub_2) %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  select(term, estimate) %>%
  filter(term != "(Intercept)") %>% 
  slice_max(order_by = estimate, n = 5)
```

:::
:::

#### Q13: Explanation

In your own words, give an explanation for what we saw in Q10 versus in Q12.

# Part Four: A final model

#### Q14: Tuning

Using `genes_sub`, tune both the `penalty` and the `mixture` term. Choose the `penalty` that is largest without losing accuracy.

```{r}
#| label: elastic-net-tune
#| cache: true

elastic_spec <- logistic_reg(penalty = tune(), 
                             mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

elastic_wflow <- workflow() %>%
  add_recipe(genes_rec) %>%
  add_model(elastic_spec)

elastic_grid <- grid_regular(
  penalty(
    c(-10, 0), 
    trans = log2_trans()
    ),
  mixture(),
  levels = 4)

tuning <- elastic_wflow %>%
  tune_grid(
    resamples = cvs,
    grid = elastic_grid
  )

tuning %>% 
  show_best(metric = "accuracy")
```

The highest penalty with the highest accuracy is 0.0992. 

#### Q15: Mixture Parameter

Interpret the selected `mixture` parameter in words.

The mixture associated with the highest penalty and accuracy is 0.333. This mixture indicates that our model is assigning approximately 1/3 of the penalty to the ridge method and 2/3 of the penalty to LASSO. This means, we are prioritizing reducing bias over variance with a mixture that gives higher weight to LASSO (which reduces bias) and lower weight to ridge (which reduces variance). 

#### Q16: Conclusion

Using the parameters you selected above, fit your model to the **full dataset**.

```{r}
#| label: elastic-net-final
#| cache: true

elastic_spec_untune <- logistic_reg(penalty = 0.0992, 
                                    mixture = 0.333) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

elastic_wflow <- workflow() %>%
  add_recipe(genes_rec_full) %>%
  add_model(elastic_spec_untune)

elastic_fit <- elastic_wflow %>%
  fit(genes) 
```

How many genes were selected? 

```{r}
#| label: elastic-net-fit-nonzero-coefficients

elastic_fit %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  count(estimate != 0) %>% 
  mutate(`estimate != 0` = if_else(`estimate != 0` == "TRUE", 
                                   "Coefficient of 0", 
                                   "Non Zero Coefficient")
         ) %>% 
  rename(`Coefficients` = `estimate != 0`, 
         Count = n)
```

Which seem to be most important?

```{r}
#| label: elastic-net-fit-top-variables

elastic_fit %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  select(-penalty) %>% 
  slice_max(order_by = estimate, n = 10)
```

Report the performance of the model on the validation set.

```{r}
#| label: elastic-net-accuracy

genes_validation %>%
  mutate(
    preds = predict(elastic_fit, genes_validation)$.pred_class
  ) %>%
  accuracy(truth = cancer,
           estimate = preds)
```

Hilariously, this is identical to the accuracy we got from using a random forest. 
