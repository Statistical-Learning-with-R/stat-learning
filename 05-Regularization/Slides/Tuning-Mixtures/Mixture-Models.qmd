---
title: "Tuning Mixtures and Penalties"
format: 
  revealjs:
    theme: ../../../style.scss
editor: source
execute: 
  echo: true
  message: false
  warning: false
embed-resources: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(tidymodels)
library(kknn)
library(glmnet)
library(here)
library(rpart.plot)
library(discrim)

set.seed(98249)
```

```{r}
#| label: data-load
#| include: false
cann <- read_csv("https://www.dropbox.com/s/s2a1uoiegitupjc/cannabis_full.csv?dl=1")

cann <- cann %>%
  dplyr::select(-Type, -Strain, -Effects, -Flavor, -Dry, -Mouth) %>%
  drop_na()

```

## Penalty Hyperparameter

Recall: In **ridge** and **LASSO** regression, we add a *penalty* term that is balanced by a parameter $\lambda$

. . .

$$\text{minimize:} \; \;  RSS + \lambda * \text{penalty}$$

## Penalty Hyperparameter

What is the "best" choice of $\lambda$?

. . .

::: {.centered}
![](https://upload.wikimedia.org/wikipedia/en/4/44/Try_Everything_%28Shakira%29.jpg)
:::


## Penalty Parameter

Last class, we tried two LASSO models:

```{r}
#| label: lasso-models-day1

lasso_1 <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

lasso_5 <- linear_reg(penalty = 0.5, mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")
```

## Penalty Parameter

We found that larger penalty parameter led to more coefficients of 0 (i.e., excluded variables):

::: columns
::: {.column width="47%"}
```{r}
#| echo: false
#| label: lasso-0.1-lambda

res1 <- lasso_1 %>%
  fit(Rating ~ ., cann) %>%
  tidy()

res1
```
:::

::: {.column width="3%"}
:::

::: {.column width="47%"}
```{r}
#| echo: false
#| label: lasso-0.5-lambda

res5 <- lasso_5 %>%
  fit(Rating ~ ., cann) %>%
  tidy()

res5
```
:::
:::

## Penalty Parameter

$\lambda = 0.1 \rightarrow$ 7 variables kept, 56 dropped

$\lambda = 0.5 \rightarrow$ 0 variables kept, 63 dropped

. . .

**Prediction:** What penalty leads to the best cross-validated prediction accuracy?

Let's use `tune()` and `tune_grid()` to find out!

## Penalty Parameter

```{r}
#| label: lambda-grid
#| echo: false
#| cache: true
#| warning: false

lam_grid <- grid_regular(penalty(), 
                         levels = 10)

cann_cvs <- vfold_cv(cann, v = 5)

lasso_spec <- linear_reg(penalty = tune(), 
                         mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

lasso_recipe <- recipe(Rating ~ ., data = cann) %>% 
  step_zv()

wflow_lasso <- workflow() %>%
  add_model(lasso_spec) %>%
  add_recipe(lasso_recipe)

tune_res <- wflow_lasso %>%
  tune_grid(
    resamples = cann_cvs,
    grid = lam_grid
  )

tune_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) +
  geom_line() +
  labs(x = "Lambda", 
       y = "", 
       title = "RMSE over Different Penalty Values")
```

## Penalty Parameter

One wrinkle in tuning: The automatic grid chooses values on a log scale, not evenly spaced:

::: columns
::: {.column width="40%"}
```{r}
#| label: lambda-log-grid

lam_grid <- grid_regular(penalty(), 
                         levels = 10)
lam_grid
```
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
```{r}
#| label: lambda-untransformed-grid

lam_grid_2 <- grid_regular(penalty(c(0, 0.5), 
                                   trans = NULL),
                           levels = 10)
lam_grid_2
```
:::
:::

## Penalty Parameter

```{r}
#| echo: false
#| cache: true
#| label: smaller-lambda-grid

cann_cvs <- vfold_cv(cann, 5)

lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

wflow_lasso <- workflow() %>%
  add_model(lasso_spec) %>%
  add_recipe(
    recipe(Rating ~ ., data = cann)
  )

tune_res <- wflow_lasso %>%
  tune_grid(
    resamples = cann_cvs,
    grid = lam_grid_2
  )

tune_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = penalty, y = mean)) +
  geom_line() + 
  labs(x = "Lambda", 
       y = "", 
       title = "RMSE over Different Penalty Values")
```

##  {background-color="#B6CADA"}

::: larger
Don't forget about interpretability!
:::

How many variables do we want to retain?

## Number of Variables Kept

```{r}
#| echo: false
#| label: counting-non-zero-coefficients

count_nonzero_vars <- function(lambda) {
  linear_reg(mixture = 1, penalty = lambda) %>%
    set_mode("regression") %>%
    set_engine("glmnet") %>%
    fit(Rating ~ ., data= cann) %>%
    tidy() %>%
    filter(estimate != 0) %>%
    nrow()
}

lam_grid_2 <- lam_grid_2 %>%
  mutate(
    num_vars = map_int(penalty, count_nonzero_vars)
  )

lam_grid_2 %>%
  ggplot(mapping = aes(x = penalty, y = num_vars)) +
  geom_col(color = "white") +
  labs(x = "Lambda Penalty", 
       y = "",
       title = "Number of Variables Retained")

```

## Penalty Parameter

Because our **regularized methods** are de-prioritizing RMSE, they will rarely give us better **prediction residuals**.

. . .

So, why do it?

. . .

</br>

**LASSO** $\rightarrow$ **Variable selection**

If we can achieve *nearly* the same predictive power with *many* fewer variables, we have a more interpretable model.

## Try it!

Open `Activity-Variable-Selection` from last class

1.  Tweak your choice of penalty in your LASSO regression until you get approximately the same number of variables as you did via **stepwise selection**.

2.  Are they the same variables?

##  {background-color="#B6CADA"}

::: larger
Model Stability
:::

## Model Stability

Consider dividing the dataset into 3 randomly split subsets.

. . .

We fit a **linear model** on *all* predictors for each subset.

. . .

Should we expect similar answers?

## Split 1

```{r}
#| echo: false
#| cache: true
#| label: lm-split-results

cann_split <- vfold_cv(cann, 3)

lm_dfs <- cann_split$splits %>%
  map(
    .f = ~ tidy(
      lm(Rating ~ ., 
         data = as.data.frame(.x)
         )
      )
    )

```

```{r}
#| label: split1-lm-model
#| echo: false
#| warning: false

lm_dfs[[1]] %>%
  slice(-1) %>%
  ggplot(mapping = aes(y = term, x = estimate)) +
  geom_col() +
  xlim(c(-0.1, 0.4)) +
  labs(y = "", 
       x = "Estimate", 
       title = "Linear Regression Coefficients for 62 Variables in Cannibas Dataset")
```

## Split 2

```{r}
#| label: split2-lm-model
#| echo: false
#| warning: false

lm_dfs[[2]] %>%
  slice(-1) %>%
  ggplot(mapping = aes(y = term, x = estimate)) +
  geom_col() +
  xlim(c(-0.1, 0.4)) +
  labs(y = "", 
       x = "Estimate", 
       title = "Linear Regression Coefficients for 62 Variables in Cannibas Dataset")

```

## Split 3

```{r}
#| label: split3-lm-model
#| echo: false
#| warning: false

lm_dfs[[3]] %>%
  slice(-1) %>%
  ggplot(mapping = aes(y = term, x = estimate)) +
  geom_col() +
  xlim(c(-0.1, 0.4)) +
  labs(y = "", 
       x = "Estimate", 
       title = "Linear Regression Coefficients for 62 Variables in Cannibas Dataset")

```

## What's happening?

When we have **many variables**, there is probably some *collinearity*.

Meaning, combinations of variables contain *the same information*.

. . .

This makes the model is very *unstable*!

So, what variables should we use?

## Ridge Regression -- Split 1

```{r, echo = FALSE,  cache = TRUE}
#| label: ridge-regression
#| echo: false
#| cache: true

ridge_spec <- linear_reg(mode = "regression", 
                         penalty = 0.3, 
                         mixture = 0) %>%
  set_engine("glmnet")

ridge_dfs <- cann_split$splits %>%
  map(
    .f = ~ fit(ridge_spec, 
               Rating ~ ., 
               data = as.data.frame(.x)
               ) %>% tidy()
    )

```


```{r}
#| label: split1-ridge-model
#| echo: false
#| warning: false

ridge_dfs[[1]] %>%
  slice(-1) %>%
  ggplot(mapping = aes(y = term, x = estimate)) +
  geom_col() +
  xlim(c(-0.1, 0.4)) +
  labs(y = "", 
       x = "Estimate", 
       title = "Ridge Regression Coefficients for 62 Variables in Cannibas Dataset")
```

## Split 2

```{r}
#| label: split2-ridge-model
#| echo: false
#| warning: false

ridge_dfs[[2]] %>%
  slice(-1) %>%
  ggplot(mapping = aes(y = term, x = estimate)) +
  geom_col() +
  xlim(c(-0.1, 0.4)) +
  labs(y = "", 
       x = "Estimate", 
       title = "Ridge Regression Coefficients for 62 Variables in Cannibas Dataset")

```

## Split 3

```{r}
#| label: split3-ridge-model
#| echo: false
#| warning: false

ridge_dfs[[3]] %>%
  slice(-1) %>%
  ggplot(mapping = aes(y = term, x = estimate)) +
  geom_col() +
  xlim(c(-0.1, 0.4)) +
  labs(y = "", 
       x = "Estimate", 
       title = "Ridge Regression Coefficients for 62 Variables in Cannibas Dataset")
```

## Ridge regression

Why does the ridge penalty help?

. . .

It reduces the **variance** of the **coefficient estimates**.

It lets all the variables "share the load" instead of putting too much weight on any one coefficient.

## There is no free lunch!

Lowering the *variance* of the estimates increases the *bias*.

In other words - we aren't prioritizing *prediction* or *RMSE* anymore. Our y-hats are not as close to our y's.

##  {background-color="#B6CADA"}

::: larger
Elastic Net
:::

## Elastic Net

What if we want to reduce the number of variables AND reduce the coefficient variance?

![](https://media1.tenor.com/images/efa0962878857ab3332344f84c41a345/tenor.gif?itemid=3787079)

## Adding Penalties

We'll just use **two** penalty terms:

$$ \text{RSS} + \frac{\lambda}{2} \times \text{(LASSO penalty)} + \frac{\lambda}{2} \times \text{(Ridge penalty)}$$

. . .

When we do half-and-half, this is called "Elastic Net".

## Mixtures of Penalties

Why half-and-half? Why not 1/3 and 2/3? 1/4 and 3/4???

. . .

$$\text{RSS} + \alpha \text{ } (\lambda \times \text{LASSO penalty}) + (1 - \alpha) (\lambda \times \text{Ridge penalty})$$ 

$\alpha$ is the `mixture` parameter.

## Try it -- Step 1

Open `Activity-Variable-Selection` from last class.

1.  Tune both the **mixture** and the **penalty** parameters.

2.  Plot the RMSE and / or R-squared across a few penalties (at one mixture)

3.  Plot the RMSE and / or R-squared across a few mixtures (at one penalty)

## Try it -- Step 2

Open [`Activity-Mixture-Models.qmd`](../../Classwork/Activity-Mixture-Models.qmd)

Recall: We wanted to predict the `Type` of cannabis from the descriptor words.

Consider only Indica vs. Sativa (remove Hybrid)

Can you combine **logistic regression** with **LASSO** to tell me which words best separate Indica and Sativa?

How does this compare to what you find with a decision tree?
